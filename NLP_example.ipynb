{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO+IBkpPXCwabHwyrOFlorq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Biswa040/Natural_Language_Process/blob/main/NLP_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtJROOZfJPL5",
        "outputId": "f0f9a80e-e48b-4b6f-81f1-23a9c843a5ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No match found!!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "text = \"I love icecream\"\n",
        "pattern = \"and\"\n",
        "match = re.search(pattern, text)\n",
        "if match:\n",
        "  print(\"Found a match:\",match.group())\n",
        "else:\n",
        "  print(\"No match found!!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I am a data analyst\"\n",
        "pattern = \"analyst\"\n",
        "replace = \"scienctist\"\n",
        "new = re.sub(pattern, replace, text)\n",
        "print(\"Original test: \",text)\n",
        "print(\"New text: \",new)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yimZyYflJ_Bo",
        "outputId": "37c263c7-31a4-4f28-feb4-6caf4879d200"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original test:  I am a data analyst\n",
            "New text:  I am a data scienctist\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = \"Hey, this is best : for ! us;\"\n",
        "print(\"The original string is :\" + test)\n",
        "res = re.sub(r'[^\\w\\s]','',test)\n",
        "print(\"The string after punctuation filter: \" + res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yzx-4aZQK8p9",
        "outputId": "8b36bbcd-1fce-4566-f2d1-db1f1d51f291"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The original string is :Hey, this is best : for ! us;\n",
            "The string after punctuation filter: Hey this is best  for  us\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('punkt')\n",
        "\n",
        "text = \"I love icecream\"\n",
        "tokens = nltk.word_tokenize(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsHXQmRSL4sF",
        "outputId": "2c7fa343-9e31-4598-c548-1f01982d50d9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'love', 'icecream']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"There was a girl named shivangi. She had a friend named deeya. They were the best!\"\n",
        "sentences = nltk.sent_tokenize(text)\n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oh_WvBQLNOuO",
        "outputId": "1ff39bc6-042e-47dd-feee-7bc376e766fd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['There was a girl named shivangi.', 'She had a friend named deeya.', 'They were the best!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import ngrams\n",
        "text = \"The quick brown fox jumps over a lazy dog\"\n",
        "tokens = nltk.word_tokenize(text)\n",
        "bigrams = list(ngrams(tokens, 2))\n",
        "trigrams = list(ngrams(tokens, 3))\n",
        "print(\"Original text: \",text)\n",
        "print(\"Tokens: \",tokens)\n",
        "print(\"Bigrams: \",bigrams)\n",
        "print(\"Trigrams: \",trigrams)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWuGwfsSOeIt",
        "outputId": "3ef2bd1f-f585-40bd-8b27-bdc3262797b0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text:  The quick brown fox jumps over a lazy dog\n",
            "Tokens:  ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'a', 'lazy', 'dog']\n",
            "Bigrams:  [('The', 'quick'), ('quick', 'brown'), ('brown', 'fox'), ('fox', 'jumps'), ('jumps', 'over'), ('over', 'a'), ('a', 'lazy'), ('lazy', 'dog')]\n",
            "Trigrams:  [('The', 'quick', 'brown'), ('quick', 'brown', 'fox'), ('brown', 'fox', 'jumps'), ('fox', 'jumps', 'over'), ('jumps', 'over', 'a'), ('over', 'a', 'lazy'), ('a', 'lazy', 'dog')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "word = \"happiness\"\n",
        "stemmed_words = stemmer.stem(word)\n",
        "print(stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPQGDQGkP1K6",
        "outputId": "9ddf8267-6040-43f8-ccb9-3af0ec26f845"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "happi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "word = \"playing\"\n",
        "lemma = lemmatizer.lemmatize(word, pos=\"v\")\n",
        "print(lemma)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KF1zgv_iQz7v",
        "outputId": "5f5dd80e-299c-471c-f436-ff4141c22e43"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "play\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens = nltk.word_tokenize(text)\n",
        "filter = [word for word in tokens if not word.lower() in stop_words]\n",
        "filtered = ' '.join(filter)\n",
        "print(\"data\",filtered)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PmsdG5CSXcN",
        "outputId": "830c793d-cc0c-4489-cac4-c968b0d5a489"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data quick brown fox jumps lazy dog\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "text =\"The quick brown fox jumps over the lazy dog.\"\n",
        "tokens = word_tokenize(text)\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "print(pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6i8vVK-MUJKE",
        "outputId": "7e537141-5992-49ff-eba4-33a848d85502"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "text = \"Apple is expected to launch a new iPhone next month.\"\n",
        "tokens = word_tokenize(text)\n",
        "tagged = pos_tag(tokens)\n",
        "ne = ne_chunk(tagged)\n",
        "\n",
        "for chunk in ne:\n",
        "  if hasattr(chunk,'label'):\n",
        "    print(chunk.label(),' '.join(c[0] for c in chunk))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVPSQg5XV7gj",
        "outputId": "c73e396d-f31b-4865-ea81-b7b556f63ccf"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPE Apple\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "text = \" I hate this product! It would not Work properly and I would definitely recommend it to anyone.\"\n",
        "scores = sid.polarity_scores(text)\n",
        "print(scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvK_i2qzYGNF",
        "outputId": "bf4682a2-d47d-40ec-8d09-df1af82ad07a"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'neg': 0.175, 'neu': 0.566, 'pos': 0.259, 'compound': 0.2003}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k4rPgTHAagzr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}